{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6bb7b9",
   "metadata": {},
   "source": [
    "# Apprentissage profond, TD1: pratique basique de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b7e86",
   "metadata": {},
   "source": [
    "## 1. Apprentissage de fizz buzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6701626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f690d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz(i: int, prediction)-> list:\n",
    "    return [ str (i) , \"fizz\" , \"buzz\" , \" fizzbuzz \"][ prediction ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eef9d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation verite terrain : [ number , \"fizz\" , \"buzz\" , \"fizzbuzz\"]\n",
    "def fizz_buzz_encode (i) :\n",
    "    if i % 3 ==  0 and i % 5 == 0:\n",
    "        return 3\n",
    "    if i % 3 == 0: # Multiples de 3 remplacés par \"fizz\"\n",
    "        return 1\n",
    "    if i % 5 == 0: # Multiples de 5 par buzz\n",
    "        return 2\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4ef09d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fizz_buzz_encode(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f5e17",
   "metadata": {},
   "source": [
    "### Modélisation des entrées et des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a777e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS = 10\n",
    "def binary_encode (i , num_digits ):\n",
    "    return np.array ([ (i >> d) & 1 for d in range ( num_digits ) ]) # L'opérateur >> décale le nombre entier $i$ de $d$ positions vers la droite. L'opérateur & (AND binaire) compare les bits du résultat de la première opération avec 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bff7ae",
   "metadata": {},
   "source": [
    "Création du corpus d'apprentissage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb56a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# données d’entraînement (X) et labels (Y)\n",
    "X_train = torch.FloatTensor ([ binary_encode(i , NUM_DIGITS ) for i in range ( 101 , 1024 )])\n",
    "Y_train = torch.LongTensor ([ fizz_buzz_encode(i) for i in range (101 , 1024)]).squeeze()\n",
    "\n",
    "# données de test\n",
    "X_test = torch.FloatTensor ([ binary_encode(i , NUM_DIGITS ) for i in range (1 , 101)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93159418",
   "metadata": {},
   "source": [
    "Création du model, de la loss et de l'optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "495a6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de neurones dans la coucge cachée\n",
    "HIDDEN_SIZE = 100\n",
    "\n",
    "# Définition du MLP à une couche cachée (non linéaire: ReLu)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS , HIDDEN_SIZE),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(HIDDEN_SIZE , 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb7965",
   "metadata": {},
   "source": [
    "On va utiliser la cross entropy comme loss en s'appuyant sur le principe du maximum de vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb2cc3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c537af",
   "metadata": {},
   "source": [
    "La cross entropy est implémenté dans pytorch aussi dans nnNLLLoss(), NLL est l'acronyme de negative log likelihood loss, elle est nommé ainsi car elle s'attends a recevoir des log-probabilités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86c4e7",
   "metadata": {},
   "source": [
    "En pratique, l'implémentation de nn.CrossEntropyLoss() dans PyTorch est en réalité une combinaison de deux étapes :$$\\text{nn.CrossEntropyLoss}(\\text{Logits}) = \\text{nn.NLLLoss}(\\text{nn.LogSoftmax}(\\text{Logits}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4ff67",
   "metadata": {},
   "source": [
    "**Avantage** : Elle est plus numériquement stable que de réaliser les deux étapes séparément, car les calculs de log-probabilités sont optimisés pour éviter les erreurs d'arrondi (comme les $\\log(0)$ ou les exponentielles de très grands nombres)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750cc8c",
   "metadata": {},
   "source": [
    "On choisit ensuite de minimiser le coût par le gradient stochastique (SGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "adb1387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b364f8c",
   "metadata": {},
   "source": [
    "Le Learning Rate ($\\text{lr}$ ou $\\eta$) est un scalaire positif qui détermine la taille du pas effectué dans la direction opposée au gradient de la fonction de perte (loss function).Il agit comme un facteur de mise à l'échelle appliqué au gradient calculé sur le mini-batch actuel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560930f",
   "metadata": {},
   "source": [
    "La règle de mise à jour pour un poids $W$ dans un $\\text{SGD}$ est donnée par : $W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\nabla J(W_{\\text{old}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826ee0f",
   "metadata": {},
   "source": [
    "$W_{\\text{new}}$ : Le nouveau poids après la mise à jour.$W_{\\text{old}}$ : Le poids actuel.$\\eta$ (ou $\\text{lr}$) : Le Learning Rate.$\\nabla J(W_{\\text{old}})$ : Le gradient de la fonction de perte $J$ par rapport aux poids $W$ (indiquant la direction de la plus forte pente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a3bb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|▎         | 28/1000 [00:00<00:07, 137.42epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 1.196959376335144\n",
      "['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44'\n",
      " '45' '46' '47' '48' '49' '50' '51' '52' 'fizz' '54' '55' '56' 'fizz' '58'\n",
      " '59' '60' 'fizz' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72'\n",
      " '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85' '86'\n",
      " '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99' '100']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|▋         | 69/1000 [00:00<00:06, 140.30epoch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Backward pass (retro-propagation)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calcul du coût (et affichage)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clemm\\miniconda3\\envs\\deeplearning-env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mregister_post_accumulate_grad_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook):\n\u001b[0;32m    636\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers a backward hook that runs after grad accumulation.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    The hook will be called after all gradients for a tensor have been accumulated,\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    meaning that the .grad field has been updated on that tensor. The post\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    accumulate grad hook is ONLY applicable for leaf tensors (tensors without a\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    .grad_fn field). Registering this hook on a non-leaf tensor will error!\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m    The hook should have the following signature::\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m        hook(param: Tensor) -> None\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03m    Note that, unlike other autograd hooks, this hook operates on the tensor\u001b[39;00m\n\u001b[1;32m--> 648\u001b[0m \u001b[38;5;124;03m    that requires grad and not the grad itself. The hook can in-place modify\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m    and access its Tensor argument, including its .grad field.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03m    This function returns a handle with a method ``handle.remove()``\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03m    that removes the hook from the module.\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \n\u001b[0;32m    654\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;124;03m        See :ref:`backward-hooks-execution` for more information on how when this hook\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;124;03m        is executed, and how its execution is ordered relative to other hooks. Since\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;124;03m        this hook runs during the backward pass, it will run in no_grad mode (unless\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m        create_graph is True). You can use torch.enable_grad() to re-enable autograd\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m        within the hook if you need it.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    Example::\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03m        >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m        >>> lr = 0.01\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m        >>> # simulate a simple SGD update\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m        >>> h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;124;03m        >>> v.backward(torch.tensor([1., 2., 3.]))\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;124;03m        >>> v\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m        tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \n\u001b[0;32m    671\u001b[0m \u001b[38;5;124;03m        >>> h.remove()  # removes the hook\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    675\u001b[0m             Tensor\u001b[38;5;241m.\u001b[39mregister_post_accumulate_grad_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook\n\u001b[0;32m    676\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\clemm\\miniconda3\\envs\\deeplearning-env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m    347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[1;32m--> 353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\clemm\\miniconda3\\envs\\deeplearning-env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "raw_data_test = np.arange(1 , 101) # Valeurs de test\n",
    "for epoch in tqdm(range(1000), desc=\"Training Epochs\", unit=\"epoch\"):\n",
    "    for start in range(0 , len (X_train) , BATCH_SIZE ):\n",
    "        end = start + BATCH_SIZE\n",
    "        batch_X = X_train[start:end]\n",
    "        batch_Y = Y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(batch_X)\n",
    "        \n",
    "        # Calcul de la loss\n",
    "        loss = loss_fn(y_pred, batch_Y)\n",
    "        \n",
    "        # Optimisation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass (retro-propagation)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calcul du coût (et affichage)\n",
    "    loss = loss_fn(model(X_train), Y_train)\n",
    "    if epoch % 100 == 0:\n",
    "        tqdm.write(f\"Epoch {epoch} - Loss: {loss.item()}\")\n",
    "    \n",
    "    # Visualisation des résultats en cours d'apprentissage\n",
    "    if epoch % 1000 == 0:\n",
    "        Y_test_pred = model(X_test)\n",
    "        val, idx = torch.max(Y_test_pred, 1)\n",
    "        ii = idx.data.numpy()\n",
    "\n",
    "        output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Final Output =============\n",
      "['1' '2' 'fizz' '4' 'buzz' '6' '7' '8' 'buzz' '10' '11' 'fizz' '13' '14'\n",
      " '15' '16' '17' '18' '19' '20' 'fizz' '22' '23' '24' 'buzz' '26' '27' '28'\n",
      " '29' '30' '31' '32' '33' '34' 'buzz' '36' '37' '38' '39' 'buzz' '41'\n",
      " 'fizz' '43' '44' ' fizzbuzz ' '46' '47' '48' '49' 'buzz' '51' '52' '53'\n",
      " '54' 'buzz' '56' '57' '58' '59' ' fizzbuzz ' '61' '62' 'fizz' '64' '65'\n",
      " 'fizz' '67' '68' '69' '70' '71' 'fizz' '73' '74' 'fizz' '76' '77' 'fizz'\n",
      " '79' '80' '81' '82' '83' 'fizz' 'buzz' '86' 'fizz' '88' '89' ' fizzbuzz '\n",
      " '91' '92' 'fizz' '94' '95' 'fizz' '97' '98' '99' 'fizz']\n",
      "============= Final Output (compact) =============\n",
      "['1', '2', 'fizz', '4', 'buzz', '6', '7', '8', 'buzz', '10', '11', 'fizz', '13', '14', '15', '16', '17', '18', '19', '20', 'fizz', '22', '23', '24', 'buzz', '26', '27', '28', '29', '30', '31', '32', '33', '34', 'buzz', '36', '37', '38', '39', 'buzz', '41', 'fizz', '43', '44', ' fizzbuzz ', '46', '47', '48', '49', 'buzz', '51', '52', '53', '54', 'buzz', '56', '57', '58', '59', ' fizzbuzz ', '61', '62', 'fizz', '64', '65', 'fizz', '67', '68', '69', '70', '71', 'fizz', '73', '74', 'fizz', '76', '77', 'fizz', '79', '80', '81', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', ' fizzbuzz ', '91', '92', 'fizz', '94', '95', 'fizz', '97', '98', '99', 'fizz']\n"
     ]
    }
   ],
   "source": [
    "# Sortie finale après entraînement (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "val, idx = torch.max(Y_test_pred, 1)\n",
    "ii = idx.data.numpy()\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============= Final Output =============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finiale (calcul plus compact des prédictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============= Final Output (compact) =============\")\n",
    "print([fizz_buzz(i, prediction) for i, prediction in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c74d9",
   "metadata": {},
   "source": [
    "Obtention du taux de classification sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe4f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "gtY = np.array([fizz_buzz_encode(i) for i in range(1, 101)])\n",
    "print(\"Accuracy:\", np.mean(ii == gtY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53504569",
   "metadata": {},
   "source": [
    "Séparation des données d'entrainement pour monitorer l'apprentissage correctement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL = 100\n",
    "p = np.random.permutation(range(len(X_train)))\n",
    "X_train, Y_train = X_train[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
